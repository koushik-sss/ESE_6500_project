{"cells":[{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"ename":"RuntimeError","evalue":"mat1 and mat2 shapes cannot be multiplied (1x14 and 15x64)","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 390\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39m# Looping until the episode ends\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    389\u001b[0m     \u001b[39m# Choosing an action based on the current state using the policy network\u001b[39;00m\n\u001b[0;32m--> 390\u001b[0m     action, log_prob \u001b[39m=\u001b[39m policy_net\u001b[39m.\u001b[39;49mact(state)\n\u001b[1;32m    392\u001b[0m     \u001b[39m# Taking the action in the environment and getting the next state, reward and done flag\u001b[39;00m\n\u001b[1;32m    393\u001b[0m     next_state, reward, done, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n","Cell \u001b[0;32mIn[12], line 256\u001b[0m, in \u001b[0;36mPolicyNetwork.act\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mact\u001b[39m(\u001b[39mself\u001b[39m, state):\n\u001b[1;32m    254\u001b[0m \u001b[39m# Choosing an action based on the current state using the policy network\u001b[39;00m\n\u001b[1;32m    255\u001b[0m   state \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(state)\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m) \u001b[39m# converting the state to a torch tensor and adding a batch dimension\u001b[39;00m\n\u001b[0;32m--> 256\u001b[0m   probs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(state)\u001b[39m.\u001b[39msqueeze() \u001b[39m# getting the probability distribution and removing the batch dimension\u001b[39;00m\n\u001b[1;32m    257\u001b[0m   probs[probs \u001b[39m!=\u001b[39m probs] \u001b[39m=\u001b[39m \u001b[39m1e-8\u001b[39m \u001b[39m# replacing any NaN values with a small positive value\u001b[39;00m\n\u001b[1;32m    258\u001b[0m   m \u001b[39m=\u001b[39m Categorical(probs) \u001b[39m# creating a categorical distribution object\u001b[39;00m\n","Cell \u001b[0;32mIn[12], line 240\u001b[0m, in \u001b[0;36mPolicyNetwork.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m    239\u001b[0m     \u001b[39m# Forward pass of the network\u001b[39;00m\n\u001b[0;32m--> 240\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc1(x)) \u001b[39m# applying ReLU activation function to the output of the first layer\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc2(x) \u001b[39m# passing the output to the second layer\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39m1e-8\u001b[39m \u001b[39m# adding a small epsilon value to avoid NaN values\u001b[39;00m\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n","\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x14 and 15x64)"]}],"source":["# Importing the necessary libraries\n","import gym\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.distributions import Categorical\n","\n","# Defining some constants for the simulation\n","GRAVITY = 9.8 # m/s^2\n","MASS = 10 # kg\n","LENGTH = 1 # m\n","RADIUS = 0.2 # m\n","INCLINATION = np.pi / 6 # rad\n","SLOPE = 0.1 # rad\n","\n","# Defining the action space as a discrete space of four actions\n","ACTION_SPACE = ['pedal forward', 'pedal backward', 'turn left', 'turn right']\n","\n","# Defining the observation space as a box space of six variables and eight pixels\n","OBS_SPACE = gym.spaces.Box(low=np.array([-5, -5, -np.pi, -np.inf, -np.inf, -np.pi, 0, 0, 0, 0, 0, 0, 0, 0, 0]), \n","                           high=np.array([5, 5, np.pi, np.inf, np.inf, np.pi, 1, 1, 1, 1, 1, 1, 1, 1, 14.14214]), \n","                           dtype=np.float32)\n","\n","# Defining a custom environment class that simulates a bicycle in 2D\n","class BicycleEnv(gym.Env):\n","    def __init__(self):\n","        # Initializing the action and observation spaces\n","        self.action_space = gym.spaces.Discrete(len(ACTION_SPACE))\n","        self.observation_space = OBS_SPACE\n","        \n","        # Initializing the state variables\n","        self.x = 0 # horizontal position\n","        self.y = 0 # vertical position\n","        self.theta = 0 # angle of the bicycle\n","        self.omega = 0 # angular velocity of the bicycle\n","        self.v = 0 # linear velocity of the bicycle\n","        self.phi = 0 # angle of the handlebar\n","        \n","        # Initializing the pixels around the bicycle\n","        self.pixels = np.zeros(8) # binary values indicating if there is an obstacle (1) or not (0) in each direction\n","\n","        # initializing the distance to the target\n","        self.distance = 0\n","        \n","        # Initializing the obstacles and the target\n","        self.obstacles = [] # list of tuples (x,y) representing the positions of the red circles\n","        self.target = None # tuple (x,y) representing the position of the green circle\n","        \n","        # Initializing the surface slope\n","        self.slope = SLOPE\n","        \n","        # Initializing the reward and done flags\n","        self.reward = 0 \n","        self.done = False\n","        \n","    def reset(self):\n","        # Resetting the state variables to random values within the observation space bounds\n","        self.x = np.random.uniform(-4.5, 4.5)\n","        self.y = np.random.uniform(-4.5, 4.5)\n","        self.theta = np.random.uniform(-np.pi/2, np.pi/2)\n","        self.omega = np.random.uniform(-np.pi/4, np.pi/4)\n","        self.v = np.random.uniform(-1, 1)\n","        self.phi = np.random.uniform(-np.pi/4, np.pi/4)\n","        # self.distance = np.sqrt((self.x - self.target[0])**2 + (self.y - self.target[1])**2) results in nonetype object is not subscriptable, so first check if all variables are not None\n","        if self.target is not None and self.x is not None and self.y is not None:\n","            self.distance = np.sqrt((self.x - self.target[0])**2 + (self.y - self.target[1])**2)\n","        else:\n","            self.distance = 14.14214\n","        \n","        # Resetting the pixels around the bicycle to zeros\n","        self.pixels.fill(0)\n","        \n","        # Resetting the obstacles and the target to random positions within the environment bounds\n","        self.obstacles.clear()\n","        for _ in range(10):\n","            x_obstacle = np.random.uniform(-4.5, 4.5)\n","            y_obstacle = np.random.uniform(-4.5, 4.5)\n","            self.obstacles.append((x_obstacle, y_obstacle))\n","        \n","        x_target = np.random.uniform(-4.5, 4.5)\n","        y_target = np.random.uniform(-4.5, 4.5)\n","        self.target = (x_target, y_target)\n","        \n","        # Resetting the reward and done flags to zero and False respectively\n","        self.reward = 0 \n","        self.done = False\n","        \n","        # Returning the initial observation as a numpy array\n","        return np.array([self.x, self.y, self.theta, self.omega,\n","                         self.v,self.phi] + list(self.pixels) + [self.distance] )\n","    \n","    def step(self, action):\n","        # Applying the action to the state variables\n","        if action == 0: # pedal forward\n","            self.v += 0.1 # increase the linear velocity by 0.1 m/s\n","        elif action == 1: # pedal backward\n","            self.v -= 0.1 # decrease the linear velocity by 0.1 m/s\n","        elif action == 2: # turn left\n","            self.phi += np.pi / 12 # increase the handlebar angle by pi/12 rad\n","        elif action == 3: # turn right\n","            self.phi -= np.pi / 12 # decrease the handlebar angle by pi/12 rad\n","        \n","        # Clamping the state variables to the observation space bounds\n","        self.v = np.clip(self.v, -1, 1)\n","        self.phi = np.clip(self.phi, -np.pi/4, np.pi/4)\n","        \n","        # Updating the state variables using the equations of motion\n","        self.x += self.v * np.cos(self.theta) * 0.1 # update the horizontal position using the linear velocity and the angle of the bicycle\n","        self.y += self.v * np.sin(self.theta) * 0.1 # update the vertical position using the linear velocity and the angle of the bicycle\n","        self.omega += (GRAVITY * np.sin(self.slope) - GRAVITY * np.cos(self.slope) * np.sin(self.theta + INCLINATION) + \n","                       MASS * RADIUS * self.v**2 * np.sin(self.phi) / LENGTH) * 0.1 # update the angular velocity using the gravity, mass, length, radius, linear velocity, handlebar angle and inclination of the bicycle\n","        self.theta += self.omega * 0.1 # update the angle of the bicycle using the angular velocity\n","        \n","        # Checking for collisions with obstacles or boundaries\n","        for obstacle in self.obstacles:\n","            if np.sqrt((self.x - obstacle[0])**2 + (self.y - obstacle[1])**2) <= RADIUS:\n","                # The bicycle has hit an obstacle\n","                self.reward -= 10 # give a negative reward of -10\n","                self.done = True # end the episode\n","        \n","        if abs(self.x) >= 5 or abs(self.y) >= 5:\n","            # The bicycle has reached a boundary\n","            self.reward -= 10 # give a negative reward of -10\n","            self.done = True # end the episode\n","        \n","        # Checking for reaching the target\n","        if np.sqrt((self.x - self.target[0])**2 + (self.y - self.target[1])**2) <= RADIUS:\n","            # The bicycle has reached the target\n","            self.reward += 100 # give a positive reward of +100\n","            self.done = True # end the episode\n","        \n","        # Checking for falling down\n","        if abs(self.theta) >= np.pi / 2:\n","            # The bicycle has fallen down\n","            self.reward -= 10 # give a negative reward of -10\n","            self.done = True # end the episode\n","        \n","        # Updating the pixels around the bicycle based on the environment state\n","        self.pixels.fill(0) # reset the pixels to zeros\n","        \n","        # Defining a helper function to check if a given position is within the environment bounds and not occupied by an obstacle or a target\n","        def is_free(x, y):\n","            if abs(x) >= 5 or abs(y) >= 5:\n","                return False\n","            \n","            for obstacle in self.obstacles:\n","                if np.sqrt((x - obstacle[0])**2 + (y - obstacle[1])**2) <= RADIUS:\n","                    return False\n","            \n","            if np.sqrt((x - self.target[0])**2 + (y - self.target[1])**2) <= RADIUS:\n","                return False\n","            \n","            return True\n","        \n","        # Checking for each direction around the bicycle\n","        if is_free(self.x + RADIUS, self.y): # right\n","            self.pixels[0] = 1\n","        if is_free(self.x + RADIUS, self.y + RADIUS): # right-up\n","            self.pixels[1] = 1\n","        if is_free(self.x, self.y + RADIUS): # up\n","            self.pixels[2] = 1\n","        if is_free(self.x - RADIUS, self.y + RADIUS): # left-up\n","            self.pixels[3] = 1\n","        if is_free(self.x - RADIUS, self.y): # left\n","            self.pixels[4] = 1\n","        if is_free(self.x - RADIUS, self.y - RADIUS): # left-down\n","            self.pixels[5] = 1\n","        if is_free(self.x, self.y - RADIUS): # down\n","            self.pixels[6] = 1\n","        if is_free(self.x + RADIUS, self.y - RADIUS): # right-down\n","            self.pixels[7] = 1\n","\n","        # Calculating the distance to the target\n","        if self.target is not None and self.x is not None and self.y is not None:\n","            self.distance = np.sqrt((self.x - self.target[0])**2 + (self.y - self.target[1])**2)\n","        else:\n","            self.distance = 14.14214\n","        \n","        \n","        # Returning the observation, reward, done flag and an empty info dictionary as a tuple\n","        return (np.array([self.x, self.y, self.theta, self.omega,\n","                          self.v,self.phi] + list(self.pixels)) + [self.distance], \n","                self.reward, \n","                self.done, \n","                {})\n","    \n","    def render(self):\n","        # Rendering the environment using matplotlib.pyplot\n","        import matplotlib.pyplot as plt\n","        \n","        # Creating a figure and an axis object\n","        fig, ax = plt.subplots()\n","        \n","        # Setting the axis limits and labels\n","        ax.set_xlim(-5, 5)\n","        ax.set_ylim(-5, 5)\n","        ax.set_xlabel('x')\n","        ax.set_ylabel('y')\n","        \n","        # Plotting the obstacles as red circles\n","        for obstacle in self.obstacles:\n","            ax.add_patch(plt.Circle(obstacle, RADIUS, color='red'))\n","        \n","        # Plotting the target as a green circle\n","        ax.add_patch(plt.Circle(self.target, RADIUS, color='green'))\n","        \n","        # Plotting the bicycle as a blue line with a black dot for the handlebar\n","        x_bike = [self.x - LENGTH / 2 * np.cos(self.theta), \n","                  self.x + LENGTH / 2 * np.cos(self.theta)]\n","        y_bike = [self.y - LENGTH / 2 * np.sin(self.theta), \n","                  self.y + LENGTH / 2 * np.sin(self.theta)]\n","        x_handle = x_bike[1] + RADIUS * np.cos(self.theta + self.phi)\n","        y_handle = y_bike[1] + RADIUS * np.sin(self.theta + self.phi)\n","        \n","        ax.plot(x_bike, y_bike, color='blue', linewidth=3)\n","        ax.plot(x_handle, y_handle, color='black', marker='o')\n","        \n","        # Showing the figure\n","        plt.show()\n","        \n","# Defining a custom policy network class that approximates the probability distribution of actions given observations using a two-layer neural network with ReLU activation function\n","class PolicyNetwork(nn.Module):\n","    def __init__(self):\n","        # Initializing the parent class\n","        super(PolicyNetwork, self).__init__()\n","        \n","        # Defining the network layers and and parameters\n","\n","\n","        self.fc1 = nn.Linear(OBS_SPACE.shape[0], 64) # first fully connected layer with 64 hidden units\n","        self.fc2 = nn.Linear(64, len(ACTION_SPACE)) # second fully connected layer with output units equal to the number of actions\n","        \n","        # Defining the optimizer\n","        self.optimizer = optim.Adam(self.parameters(), lr=0.01) # using Adam optimizer with learning rate of 0.01\n","        \n","    def forward(self, x):\n","        # Forward pass of the network\n","        x = F.relu(self.fc1(x)) # applying ReLU activation function to the output of the first layer\n","        x = self.fc2(x) # passing the output to the second layer\n","        x = x + 1e-8 # adding a small epsilon value to avoid NaN values\n","        return F.log_softmax(x, dim=-1).exp() # applying softmax function to get a probability distribution over actions\n","    \n","    # def act(self, state):\n","    #     # Choosing an action based on the current state\n","    #     state = torch.from_numpy(state).float().unsqueeze(0) # converting the state to a torch tensor and adding a batch dimension\n","    #     probs = self.forward(state).squeeze() # getting the probability distribution and removing the batch dimension\n","    #     m = Categorical(probs) # creating a categorical distribution object\n","    #     action = m.sample() # sampling an action from the distribution\n","    #     return action.item(), m.log_prob(action) # returning the action and its log probability\n","    \n","    def act(self, state):\n","    # Choosing an action based on the current state using the policy network\n","      state = torch.from_numpy(state).float().unsqueeze(0) # converting the state to a torch tensor and adding a batch dimension\n","      probs = self.forward(state).squeeze() # getting the probability distribution and removing the batch dimension\n","      probs[probs != probs] = 1e-8 # replacing any NaN values with a small positive value\n","      m = Categorical(probs) # creating a categorical distribution object\n","      action = m.sample() # sampling an action from the distribution\n","      return action.item(), m.log_prob(action) # returning the action and its log probability\n","\n","\n","# Defining a custom value network class that approximates the expected return given observations using a two-layer neural network with ReLU activation function\n","class ValueNetwork(nn.Module):\n","    def __init__(self):\n","        # Initializing the parent class\n","        super(ValueNetwork, self).__init__()\n","\n","        # print (OBS_SPACE.shape[0]) : the output is 15\n","\n","        \n","        # Defining the network layers and parameters\n","        self.fc1 = nn.Linear(OBS_SPACE.shape[0], 64) # first fully connected layer with 64 hidden units\n","        self.fc2 = nn.Linear(64, 1) # second fully connected layer with output unit equal to one\n","        \n","        \n","        # Defining the optimizer\n","        self.optimizer = optim.Adam(self.parameters(), lr=0.01) # using Adam optimizer with learning rate of 0.01\n","        \n","    def forward(self, x):\n","        # Forward pass of the network\n","        x = F.relu(self.fc1(x)) # applying ReLU activation function to the output of the first layer\n","        x = self.fc2(x) # passing the output to the second layer\n","        return x # returning the expected return\n","    \n","# Defining some hyperparameters for the proximal policy optimization algorithm\n","GAMMA = 0.99 # discount factor for future rewards\n","LAMBDA = 0.95 # parameter for generalized advantage estimation\n","EPSILON = 0.2 # parameter for clipping the ratio of probabilities\n","BATCH_SIZE = 64 # batch size for mini-batch updates\n","EPOCHS = 10 # number of epochs for each update\n","\n","# Creating an instance of the custom environment\n","env = BicycleEnv()\n","\n","# Creating an instance of the policy network\n","policy_net = PolicyNetwork()\n","\n","# Creating an instance of the value network\n","value_net = ValueNetwork()\n","\n","# Defining some variables to store the trajectories and statistics\n","states = [] # list of states visited by the agent\n","actions = [] # list of actions taken by the agent\n","rewards = [] # list of rewards received by the agent\n","log_probs = [] # list of log probabilities of actions taken by the agent\n","values = [] # list of expected returns estimated by the value network\n","returns = [] # list of actual returns calculated from rewards\n","advantages = [] # list of advantages calculated from returns and values\n","\n","episode_reward = 0 # cumulative reward for each episode\n","episode_rewards = [] # list of episode rewards\n","\n","# Defining a helper function to calculate discounted returns from rewards\n","def calculate_returns(rewards):\n","    returns = []\n","    R = 0 \n","    for r in reversed(rewards):\n","        R = r + GAMMA * R \n","        returns.insert(0, R)\n","    return returns\n","\n","# Defining a helper function to calculate generalized advantages from returns and values\n","def calculate_advantages(returns, values):\n","    advantages = []\n","    A = 0 \n","    for i in reversed(range(len(returns))):\n","        delta = returns[i] - values[i]\n","        A = delta + GAMMA * LAMBDA * A \n","        advantages.insert(0, A)\n","    return advantages\n","\n","# Defining a helper function to update the policy and value networks using mini-batch gradient descent\n","def update_networks(states, actions, log_probs, returns, advantages):\n","    # Converting the trajectories to torch tensors\n","    states = torch.tensor(states, dtype=torch.float)\n","    actions = torch.tensor(actions)\n","    log_probs = torch.tensor(log_probs)\n","    returns = torch.tensor(returns, dtype=torch.float)\n","    advantages = torch.tensor(advantages, dtype=torch.float)\n","    \n","    # Normalizing the advantages\n","    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n","    \n","    # Creating a dataset and a data loader from the trajectories\n","    dataset = torch.utils.data.TensorDataset(states, actions, log_probs, returns, advantages)\n","    data_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n","    \n","    # Looping over the number of epochs\n","    for _ in range(EPOCHS):\n","        # Looping over the mini-batches\n","        for state_batch, action_batch, log_prob_batch, return_batch, advantage_batch in data_loader:\n","            # Calculating the current log probabilities and values for the mini-batch states\n","            # current_log_probs = policy_net(state_batch).log_prob(action_batch).unsqueeze(-1)\n","            current_probs = policy_net(state_batch)\n","            current_log_probs = torch.log(current_probs.gather(1, action_batch.unsqueeze(-1)))\n","\n","\n","            current_values = value_net(state_batch)\n","            \n","            # Calculating the ratio of probabilities\n","            ratio = torch.exp(current_log_probs - log_prob_batch)\n","            \n","            # Calculating the surrogate losses for the policy and value networks\n","            policy_loss = -torch.min(ratio * advantage_batch,\n","                                     torch.clamp(ratio, 1 - EPSILON, 1 + EPSILON) * advantage_batch).mean()\n","            value_loss = F.mse_loss(current_values, return_batch)\n","            \n","            # Updating the policy network parameters\n","            policy_net.optimizer.zero_grad()\n","            policy_loss.backward()\n","            policy_net.optimizer.step()\n","            \n","            # Updating the value network parameters\n","            value_net.optimizer.zero_grad()\n","            value_loss.backward()\n","            value_net.optimizer.step()\n","\n","# Defining the number of episodes to train the agent\n","NUM_EPISODES = 100000\n","\n","# Looping over the episodes\n","for i in range(NUM_EPISODES):\n","    # Resetting the environment and getting the initial state\n","    state = env.reset()\n","    \n","    # Looping until the episode ends\n","    while True:\n","        # Choosing an action based on the current state using the policy network\n","        action, log_prob = policy_net.act(state)\n","        \n","        # Taking the action in the environment and getting the next state, reward and done flag\n","        next_state, reward, done, _ = env.step(action)\n","        \n","        # Estimating the expected return for the current state using the value network\n","        value = value_net(torch.from_numpy(state).float().unsqueeze(0)).item()\n","        \n","        # Storing the state, action, reward, log probability and value in the trajectories\n","        states.append(state)\n","        actions.append(action)\n","        rewards.append(reward)\n","        log_probs.append(log_prob)\n","        values.append(value)\n","        \n","        # Updating the episode reward\n","        episode_reward += reward\n","        \n","        # Updating the state\n","        state = next_state\n","        \n","        # Rendering the environment if it is a multiple of 100 episodes\n","        if (i + 1) % 100 == 0:\n","            env.render()\n","        \n","        # Checking if the episode has ended\n","        if done:\n","            # Calculating the discounted returns and generalized advantages from the trajectories\n","            returns = calculate_returns(rewards)\n","            advantages = calculate_advantages(returns, values)\n","            \n","            # Updating the policy and value networks using the trajectories\n","            update_networks(states, actions, log_probs, returns, advantages)\n","            \n","            # Clearing the trajectories and statistics\n","            states.clear()\n","            actions.clear()\n","            rewards.clear()\n","            log_probs.clear()\n","            values.clear()\n","            returns.clear()\n","            advantages.clear()\n","            \n","            # Storing the episode reward in the list\n","            episode_rewards.append(episode_reward)\n","            \n","            # Printing the episode reward\n","            print(f'Episode {i + 1}: Reward = {episode_reward}')\n","            \n","            # Resetting the episode reward\n","            episode_reward = 0\n","            \n","            # Breaking out of the loop\n","            break\n","\n","# Plotting the episode rewards over time\n","import matplotlib.pyplot as plt\n","\n","plt.plot(episode_rewards)\n","plt.xlabel('Episode')\n","plt.ylabel('Reward')\n","plt.title('Episode Rewards over Time')\n","plt.show()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
