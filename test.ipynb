{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Episode 11058: Reward = -10\n","Episode 11059: Reward = -10\n","Episode 11060: Reward = -10\n","Episode 11061: Reward = -10\n","Episode 11062: Reward = -10\n","Episode 11063: Reward = -10\n","Episode 11064: Reward = -10\n","Episode 11065: Reward = -10\n","Episode 11066: Reward = -10\n","Episode 11067: Reward = -10\n","Episode 11068: Reward = -10\n","Episode 11069: Reward = -10\n","Episode 11070: Reward = -10\n","Episode 11071: Reward = -10\n","Episode 11072: Reward = -10\n","Episode 11073: Reward = -10\n","Episode 11074: Reward = -10\n","Episode 11075: Reward = -10\n","Episode 11076: Reward = -10\n","Episode 11077: Reward = -10\n","Episode 11078: Reward = -10\n","Episode 11079: Reward = -10\n","Episode 11080: Reward = -10\n","Episode 11081: Reward = -10\n","Episode 11082: Reward = -10\n","Episode 11083: Reward = -10\n","Episode 11084: Reward = -10\n","Episode 11085: Reward = -10\n","Episode 11086: Reward = -10\n","Episode 11087: Reward = -10\n","Episode 11088: Reward = -10\n","Episode 11089: Reward = -10\n","Episode 11090: Reward = -10\n","Episode 11091: Reward = -10\n","Episode 11092: Reward = -10\n","Episode 11093: Reward = -10\n","Episode 11094: Reward = -10\n","Episode 11095: Reward = -10\n","Episode 11096: Reward = -10\n","Episode 11097: Reward = -10\n","Episode 11098: Reward = -10\n","Episode 11099: Reward = -10\n","Episode 11100: Reward = -10\n","Episode 11101: Reward = -10\n","Episode 11102: Reward = -10\n","Episode 11103: Reward = -10\n","Episode 11104: Reward = -10\n","Episode 11105: Reward = -10\n","Episode 11106: Reward = -10\n","Episode 11107: Reward = -10\n","Episode 11108: Reward = -10\n","Episode 11109: Reward = -10\n","Episode 11110: Reward = -10\n","Episode 11111: Reward = -10\n","Episode 11112: Reward = -10\n","Episode 11113: Reward = -10\n","Episode 11114: Reward = -10\n","Episode 11115: Reward = -10\n","Episode 11116: Reward = -10\n","Episode 11117: Reward = -10\n","Episode 11118: Reward = -10\n","Episode 11119: Reward = -10\n","Episode 11120: Reward = -10\n","Episode 11121: Reward = -10\n","Episode 11122: Reward = -10\n","Episode 11123: Reward = -10\n","Episode 11124: Reward = -10\n","Episode 11125: Reward = -10\n","Episode 11126: Reward = -10\n","Episode 11127: Reward = -10\n","Episode 11128: Reward = -10\n","Episode 11129: Reward = -10\n","Episode 11130: Reward = -10\n","Episode 11131: Reward = -10\n","Episode 11132: Reward = -10\n","Episode 11133: Reward = -10\n","Episode 11134: Reward = -10\n","Episode 11135: Reward = -10\n","Episode 11136: Reward = -10\n","Episode 11137: Reward = -10\n","Episode 11138: Reward = -10\n","Episode 11139: Reward = -10\n","Episode 11140: Reward = -10\n","Episode 11141: Reward = -10\n","Episode 11142: Reward = -10\n","Episode 11143: Reward = -10\n","Episode 11144: Reward = -10\n","Episode 11145: Reward = -10\n","Episode 11146: Reward = -10\n","Episode 11147: Reward = -10\n","Episode 11148: Reward = -10\n","Episode 11149: Reward = -10\n","Episode 11150: Reward = -10\n","Episode 11151: Reward = -10\n","Episode 11152: Reward = -10\n","Episode 11153: Reward = -10\n","Episode 11154: Reward = -10\n","Episode 11155: Reward = -10\n","Episode 11156: Reward = -10\n","Episode 11157: Reward = -10\n","Episode 11158: Reward = -10\n","Episode 11159: Reward = -10\n","Episode 11160: Reward = -10\n","Episode 11161: Reward = -10\n","Episode 11162: Reward = -10\n","Episode 11163: Reward = -10\n","Episode 11164: Reward = -10\n","Episode 11165: Reward = -10\n","Episode 11166: Reward = -10\n","Episode 11167: Reward = -10\n","Episode 11168: Reward = -10\n","Episode 11169: Reward = -10\n","Episode 11170: Reward = -10\n","Episode 11171: Reward = -10\n","Episode 11172: Reward = -10\n","Episode 11173: Reward = -10\n","Episode 11174: Reward = -10\n","Episode 11175: Reward = -10\n","Episode 11176: Reward = -10\n","Episode 11177: Reward = -10\n","Episode 11178: Reward = -10\n","Episode 11179: Reward = -10\n","Episode 11180: Reward = -10\n","Episode 11181: Reward = -10\n","Episode 11182: Reward = -10\n","Episode 11183: Reward = -10\n","Episode 11184: Reward = -10\n","Episode 11185: Reward = -10\n","Episode 11186: Reward = -10\n","Episode 11187: Reward = -10\n","Episode 11188: Reward = -10\n","Episode 11189: Reward = -10\n","Episode 11190: Reward = -10\n","Episode 11191: Reward = -10\n","Episode 11192: Reward = -10\n","Episode 11193: Reward = -10\n","Episode 11194: Reward = -10\n","Episode 11195: Reward = -10\n","Episode 11196: Reward = -10\n","Episode 11197: Reward = -10\n","Episode 11198: Reward = -10\n","Episode 11199: Reward = -10\n","Episode 11200: Reward = -10\n","Episode 11201: Reward = -10\n","Episode 11202: Reward = -10\n","Episode 11203: Reward = -10\n","Episode 11204: Reward = -10\n","Episode 11205: Reward = -10\n","Episode 11206: Reward = -10\n","Episode 11207: Reward = -10\n","Episode 11208: Reward = -10\n","Episode 11209: Reward = -10\n","Episode 11210: Reward = -10\n","Episode 11211: Reward = -10\n","Episode 11212: Reward = -10\n","Episode 11213: Reward = -10\n","Episode 11214: Reward = -10\n","Episode 11215: Reward = -10\n","Episode 11216: Reward = -10\n","Episode 11217: Reward = -10\n","Episode 11218: Reward = -10\n","Episode 11219: Reward = -10\n","Episode 11220: Reward = -10\n","Episode 11221: Reward = -10\n","Episode 11222: Reward = -10\n","Episode 11223: Reward = -10\n","Episode 11224: Reward = -10\n","Episode 11225: Reward = -10\n","Episode 11226: Reward = -10\n","Episode 11227: Reward = -10\n","Episode 11228: Reward = -10\n","Episode 11229: Reward = -10\n","Episode 11230: Reward = -10\n","Episode 11231: Reward = -10\n","Episode 11232: Reward = -10\n"]}],"source":["# Importing the necessary libraries\n","import gym\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.distributions import Categorical\n","\n","# Defining some constants for the simulation\n","GRAVITY = 9.8 # m/s^2\n","MASS = 10 # kg\n","LENGTH = 1 # m\n","RADIUS = 0.2 # m\n","INCLINATION = np.pi / 6 # rad\n","SLOPE = 0.1 # rad\n","\n","# Defining the action space as a discrete space of four actions\n","ACTION_SPACE = ['pedal forward', 'pedal backward', 'turn left', 'turn right']\n","\n","# Defining the observation space as a box space of six variables and eight pixels\n","OBS_SPACE = gym.spaces.Box(low=np.array([-5, -5, -np.pi, -np.inf, -np.inf, -np.pi, 0, 0, 0, 0, 0, 0, 0, 0, 0]), \n","                           high=np.array([5, 5, np.pi, np.inf, np.inf, np.pi, 1, 1, 1, 1, 1, 1, 1, 1, 14.14214]), \n","                           dtype=np.float32)\n","\n","# Defining a custom environment class that simulates a bicycle in 2D\n","class BicycleEnv(gym.Env):\n","    def __init__(self):\n","        # Initializing the action and observation spaces\n","        self.action_space = gym.spaces.Discrete(len(ACTION_SPACE))\n","        self.observation_space = OBS_SPACE\n","        \n","        # Initializing the state variables\n","        self.x = 0 # horizontal position\n","        self.y = 0 # vertical position\n","        self.theta = 0 # angle of the bicycle\n","        self.omega = 0 # angular velocity of the bicycle\n","        self.v = 0 # linear velocity of the bicycle\n","        self.phi = 0 # angle of the handlebar\n","        \n","        # Initializing the pixels around the bicycle\n","        self.pixels = np.zeros(8) # binary values indicating if there is an obstacle (1) or not (0) in each direction\n","\n","        # initializing the distance to the target\n","        self.distance = 0\n","        \n","        # Initializing the obstacles and the target\n","        self.obstacles = [] # list of tuples (x,y) representing the positions of the red circles\n","        self.target = None # tuple (x,y) representing the position of the green circle\n","        \n","        # Initializing the surface slope\n","        self.slope = SLOPE\n","        \n","        # Initializing the reward and done flags\n","        self.reward = 0 \n","        self.done = False\n","        \n","    def reset(self):\n","        # Resetting the state variables to random values within the observation space bounds\n","        self.x = np.random.uniform(-4.5, 4.5)\n","        self.y = np.random.uniform(-4.5, 4.5)\n","        self.theta = np.random.uniform(-np.pi/2, np.pi/2)\n","        self.omega = np.random.uniform(-np.pi/4, np.pi/4)\n","        self.v = np.random.uniform(-1, 1)\n","        self.phi = np.random.uniform(-np.pi/4, np.pi/4)\n","        # self.distance = np.sqrt((self.x - self.target[0])**2 + (self.y - self.target[1])**2) results in nonetype object is not subscriptable, so first check if all variables are not None\n","        if self.target is not None and self.x is not None and self.y is not None:\n","            self.distance = np.sqrt((self.x - self.target[0])**2 + (self.y - self.target[1])**2)\n","        else:\n","            self.distance = 14.14214\n","        \n","        # Resetting the pixels around the bicycle to zeros\n","        self.pixels.fill(0)\n","        \n","        # Resetting the obstacles and the target to random positions within the environment bounds\n","        self.obstacles.clear()\n","        for _ in range(10):\n","            x_obstacle = np.random.uniform(-4.5, 4.5)\n","            y_obstacle = np.random.uniform(-4.5, 4.5)\n","            self.obstacles.append((x_obstacle, y_obstacle))\n","        \n","        x_target = np.random.uniform(-4.5, 4.5)\n","        y_target = np.random.uniform(-4.5, 4.5)\n","        self.target = (x_target, y_target)\n","        \n","        # Resetting the reward and done flags to zero and False respectively\n","        self.reward = 0 \n","        self.done = False\n","        \n","        # Returning the initial observation as a numpy array\n","        # return np.array([self.x, self.y, self.theta, self.omega,\n","        #                  self.v,self.phi] + list(self.pixels) )\n","\n","        # append the distance which is a float to the returned array after self.pixels\n","\n","        # print dimensions of the returned array    \n","        # print (\"return 1 out is \")\n","        # print(np.array([self.x, self.y, self.theta, self.omega,\n","        #                  self.v,self.phi] + list(self.pixels) + [self.distance]).shape)\n","        return np.array([self.x, self.y, self.theta, self.omega,\n","                            self.v,self.phi] + list(self.pixels) + [self.distance])\n","    \n","    def step(self, action):\n","        # Applying the action to the state variables\n","        if action == 0: # pedal forward\n","            self.v += 0.1 # increase the linear velocity by 0.1 m/s\n","        elif action == 1: # pedal backward\n","            self.v -= 0.1 # decrease the linear velocity by 0.1 m/s\n","        elif action == 2: # turn left\n","            self.phi += np.pi / 12 # increase the handlebar angle by pi/12 rad\n","        elif action == 3: # turn right\n","            self.phi -= np.pi / 12 # decrease the handlebar angle by pi/12 rad\n","        \n","        # Clamping the state variables to the observation space bounds\n","        self.v = np.clip(self.v, -1, 1)\n","        self.phi = np.clip(self.phi, -np.pi/4, np.pi/4)\n","        \n","        # Updating the state variables using the equations of motion\n","        self.x += self.v * np.cos(self.theta) * 0.1 # update the horizontal position using the linear velocity and the angle of the bicycle\n","        self.y += self.v * np.sin(self.theta) * 0.1 # update the vertical position using the linear velocity and the angle of the bicycle\n","        self.omega += (GRAVITY * np.sin(self.slope) - GRAVITY * np.cos(self.slope) * np.sin(self.theta + INCLINATION) + \n","                       MASS * RADIUS * self.v**2 * np.sin(self.phi) / LENGTH) * 0.1 # update the angular velocity using the gravity, mass, length, radius, linear velocity, handlebar angle and inclination of the bicycle\n","        self.theta += self.omega * 0.1 # update the angle of the bicycle using the angular velocity\n","        \n","        # Checking for collisions with obstacles or boundaries\n","        for obstacle in self.obstacles:\n","            if np.sqrt((self.x - obstacle[0])**2 + (self.y - obstacle[1])**2) <= RADIUS:\n","                # The bicycle has hit an obstacle\n","                self.reward -= 10 # give a negative reward of -10\n","                self.done = True # end the episode\n","        \n","        if abs(self.x) >= 5 or abs(self.y) >= 5:\n","            # The bicycle has reached a boundary\n","            self.reward -= 10 # give a negative reward of -10\n","            self.done = True # end the episode\n","        \n","        # Checking for reaching the target\n","        if np.sqrt((self.x - self.target[0])**2 + (self.y - self.target[1])**2) <= RADIUS:\n","            # The bicycle has reached the target\n","            self.reward += 100 # give a positive reward of +100\n","            self.done = True # end the episode\n","        \n","        # Checking for falling down\n","        if abs(self.theta) >= np.pi / 2:\n","            # The bicycle has fallen down\n","            self.reward -= 10 # give a negative reward of -10\n","            self.done = True # end the episode\n","        \n","        # Updating the pixels around the bicycle based on the environment state\n","        self.pixels.fill(0) # reset the pixels to zeros\n","        \n","        # Defining a helper function to check if a given position is within the environment bounds and not occupied by an obstacle or a target\n","        def is_free(x, y):\n","            if abs(x) >= 5 or abs(y) >= 5:\n","                return False\n","            \n","            for obstacle in self.obstacles:\n","                if np.sqrt((x - obstacle[0])**2 + (y - obstacle[1])**2) <= RADIUS:\n","                    return False\n","            \n","            if np.sqrt((x - self.target[0])**2 + (y - self.target[1])**2) <= RADIUS:\n","                return False\n","            \n","            return True\n","        \n","        # Checking for each direction around the bicycle\n","        if is_free(self.x + RADIUS, self.y): # right\n","            self.pixels[0] = 1\n","        if is_free(self.x + RADIUS, self.y + RADIUS): # right-up\n","            self.pixels[1] = 1\n","        if is_free(self.x, self.y + RADIUS): # up\n","            self.pixels[2] = 1\n","        if is_free(self.x - RADIUS, self.y + RADIUS): # left-up\n","            self.pixels[3] = 1\n","        if is_free(self.x - RADIUS, self.y): # left\n","            self.pixels[4] = 1\n","        if is_free(self.x - RADIUS, self.y - RADIUS): # left-down\n","            self.pixels[5] = 1\n","        if is_free(self.x, self.y - RADIUS): # down\n","            self.pixels[6] = 1\n","        if is_free(self.x + RADIUS, self.y - RADIUS): # right-down\n","            self.pixels[7] = 1\n","\n","        # Calculating the distance to the target\n","        if self.target is not None and self.x is not None and self.y is not None:\n","            self.distance = np.sqrt((self.x - self.target[0])**2 + (self.y - self.target[1])**2)\n","        else:\n","            self.distance = 14.14214\n","        \n","        \n","        # Returning the observation, reward, done flag and an empty info dictionary as a tuple\n","        # return (np.array([self.x, self.y, self.theta, self.omega,\n","        #                   self.v,self.phi] + list(self.pixels)) , \n","        #         self.reward, \n","        #         self.done, \n","        #         {})\n","\n","        # append  distance, which is a float value, to the returned array. distance is appended after the pixels but before the reward\n","        return (np.array([self.x, self.y, self.theta, self.omega,\n","                            self.v,self.phi] + list(self.pixels) + [self.distance]),\n","                self.reward,\n","                self.done,\n","                {})\n","    \n","    \n","    def render(self):\n","        # Rendering the environment using matplotlib.pyplot\n","        import matplotlib.pyplot as plt\n","\n","        # clear previous plot\n","        plt.clf()\n","        \n","        \n","        # Creating a figure and an axis object\n","        fig, ax = plt.subplots()\n","        \n","        # Setting the axis limits and labels\n","        ax.set_xlim(-5, 5)\n","        ax.set_ylim(-5, 5)\n","        ax.set_xlabel('x')\n","        ax.set_ylabel('y')\n","        \n","        # Plotting the obstacles as red circles\n","        for obstacle in self.obstacles:\n","            ax.add_patch(plt.Circle(obstacle, RADIUS, color='red'))\n","        \n","        # Plotting the target as a green circle\n","        ax.add_patch(plt.Circle(self.target, RADIUS, color='green'))\n","        \n","        # Plotting the bicycle as a blue line with a black dot for the handlebar\n","        x_bike = [self.x - LENGTH / 2 * np.cos(self.theta), \n","                  self.x + LENGTH / 2 * np.cos(self.theta)]\n","        y_bike = [self.y - LENGTH / 2 * np.sin(self.theta), \n","                  self.y + LENGTH / 2 * np.sin(self.theta)]\n","        x_handle = x_bike[1] + RADIUS * np.cos(self.theta + self.phi)\n","        y_handle = y_bike[1] + RADIUS * np.sin(self.theta + self.phi)\n","        \n","        ax.plot(x_bike, y_bike, color='blue', linewidth=3)\n","        ax.plot(x_handle, y_handle, color='black', marker='o')\n","        \n","        # Showing the figure\n","        plt.show()\n","        \n","# Defining a custom policy network class that approximates the probability distribution of actions given observations using a two-layer neural network with ReLU activation function\n","class PolicyNetwork(nn.Module):\n","    def __init__(self):\n","        # Initializing the parent class\n","        super(PolicyNetwork, self).__init__()\n","        \n","        # Defining the network layers and and parameters\n","\n","\n","        self.fc1 = nn.Linear(OBS_SPACE.shape[0], 64) # first fully connected layer with 64 hidden units\n","        self.fc2 = nn.Linear(64, len(ACTION_SPACE)) # second fully connected layer with output units equal to the number of actions\n","        \n","        # Defining the optimizer\n","        self.optimizer = optim.Adam(self.parameters(), lr=0.01) # using Adam optimizer with learning rate of 0.01\n","        \n","    def forward(self, x):\n","        # Forward pass of the network\n","        x = F.relu(self.fc1(x)) # applying ReLU activation function to the output of the first layer\n","        x = self.fc2(x) # passing the output to the second layer\n","        x = x + 1e-8 # adding a small epsilon value to avoid NaN values\n","\n","        # PRINT the shape of x\n","\n","        return F.log_softmax(x, dim=-1).exp() # applying softmax function to get a probability distribution over actions\n","    \n","    # def act(self, state):\n","    #     # Choosing an action based on the current state\n","    #     state = torch.from_numpy(state).float().unsqueeze(0) # converting the state to a torch tensor and adding a batch dimension\n","    #     probs = self.forward(state).squeeze() # getting the probability distribution and removing the batch dimension\n","    #     m = Categorical(probs) # creating a categorical distribution object\n","    #     action = m.sample() # sampling an action from the distribution\n","    #     return action.item(), m.log_prob(action) # returning the action and its log probability\n","    \n","    def act(self, state):\n","    # Choosing an action based on the current state using the policy network\n","      state = torch.from_numpy(state).float().unsqueeze(0) # converting the state to a torch tensor and adding a batch dimension\n","      probs = self.forward(state).squeeze() # getting the probability distribution and removing the batch dimension\n","      probs[probs != probs] = 1e-8 # replacing any NaN values with a small positive value\n","      m = Categorical(probs) # creating a categorical distribution object\n","      action = m.sample() # sampling an action from the \n","      # print the dimension of action\n","    #   print (\" action shape\")\n","    #   print (action.shape)\n","    \n","\n","      return action.item(), m.log_prob(action) # returning the action and its log probability\n","\n","\n","# Defining a custom value network class that approximates the expected return given observations using a two-layer neural network with ReLU activation function\n","class ValueNetwork(nn.Module):\n","    def __init__(self):\n","        # Initializing the parent class\n","        super(ValueNetwork, self).__init__()\n","\n","        # print (OBS_SPACE.shape[0]) : the output is 15\n","\n","        \n","        # Defining the network layers and parameters\n","        self.fc1 = nn.Linear(OBS_SPACE.shape[0], 64) # first fully connected layer with 64 hidden units\n","        self.fc2 = nn.Linear(64, 1) # second fully connected layer with output unit equal to one\n","        \n","        \n","        # Defining the optimizer\n","        self.optimizer = optim.Adam(self.parameters(), lr=0.01) # using Adam optimizer with learning rate of 0.01\n","        \n","    def forward(self, x):\n","        # Forward pass of the network\n","        x = F.relu(self.fc1(x)) # applying ReLU activation function to the output of the first layer\n","        x = self.fc2(x) # passing the output to the second layer\n","        return x # returning the expected return\n","    \n","# Defining some hyperparameters for the proximal policy optimization algorithm\n","GAMMA = 0.99 # discount factor for future rewards\n","LAMBDA = 0.95 # parameter for generalized advantage estimation\n","EPSILON = 0.2 # parameter for clipping the ratio of probabilities\n","BATCH_SIZE = 64 # batch size for mini-batch updates\n","EPOCHS = 10 # number of epochs for each update\n","\n","# Creating an instance of the custom environment\n","env = BicycleEnv()\n","\n","# Creating an instance of the policy network\n","policy_net = PolicyNetwork()\n","\n","# Creating an instance of the value network\n","value_net = ValueNetwork()\n","\n","# Defining some variables to store the trajectories and statistics\n","states = [] # list of states visited by the agent\n","actions = [] # list of actions taken by the agent\n","rewards = [] # list of rewards received by the agent\n","log_probs = [] # list of log probabilities of actions taken by the agent\n","values = [] # list of expected returns estimated by the value network\n","returns = [] # list of actual returns calculated from rewards\n","advantages = [] # list of advantages calculated from returns and values\n","\n","episode_reward = 0 # cumulative reward for each episode\n","episode_rewards = [] # list of episode rewards\n","\n","# Defining a helper function to calculate discounted returns from rewards\n","def calculate_returns(rewards):\n","    returns = []\n","    R = 0 \n","    for r in reversed(rewards):\n","        R = r + GAMMA * R \n","        returns.insert(0, R)\n","    return returns\n","\n","# Defining a helper function to calculate generalized advantages from returns and values\n","def calculate_advantages(returns, values):\n","    advantages = []\n","    A = 0 \n","    for i in reversed(range(len(returns))):\n","        delta = returns[i] - values[i]\n","        A = delta + GAMMA * LAMBDA * A \n","        advantages.insert(0, A)\n","    return advantages\n","\n","# Defining a helper function to update the policy and value networks using mini-batch gradient descent\n","def update_networks(states, actions, log_probs, returns, advantages):\n","    # Converting the trajectories to torch tensors\n","    states = torch.tensor(states, dtype=torch.float)\n","    actions = torch.tensor(actions)\n","    log_probs = torch.tensor(log_probs)\n","    returns = torch.tensor(returns, dtype=torch.float)\n","    advantages = torch.tensor(advantages, dtype=torch.float)\n","    \n","    # Normalizing the advantages\n","    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n","    \n","    # Creating a dataset and a data loader from the trajectories\n","    dataset = torch.utils.data.TensorDataset(states, actions, log_probs, returns, advantages)\n","    data_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n","    \n","    # Looping over the number of epochs\n","    for _ in range(EPOCHS):\n","        # Looping over the mini-batches\n","        for state_batch, action_batch, log_prob_batch, return_batch, advantage_batch in data_loader:\n","            # Calculating the current log probabilities and values for the mini-batch states\n","            # current_log_probs = policy_net(state_batch).log_prob(action_batch).unsqueeze(-1)\n","            current_probs = policy_net(state_batch)\n","            current_log_probs = torch.log(current_probs.gather(1, action_batch.unsqueeze(-1)))\n","\n","\n","            current_values = value_net(state_batch)\n","            \n","            # Calculating the ratio of probabilities\n","            ratio = torch.exp(current_log_probs - log_prob_batch)\n","            \n","            # Calculating the surrogate losses for the policy and value networks\n","            policy_loss = -torch.min(ratio * advantage_batch,\n","                                     torch.clamp(ratio, 1 - EPSILON, 1 + EPSILON) * advantage_batch).mean()\n","            value_loss = F.mse_loss(current_values, return_batch)\n","            \n","            # Updating the policy network parameters\n","            policy_net.optimizer.zero_grad()\n","            policy_loss.backward()\n","            policy_net.optimizer.step()\n","            \n","            # Updating the value network parameters\n","            value_net.optimizer.zero_grad()\n","            value_loss.backward()\n","            value_net.optimizer.step()\n","\n","# Defining the number of episodes to train the agent\n","NUM_EPISODES = 100000\n","\n","# Looping over the episodes\n","for i in range(NUM_EPISODES):\n","    # Resetting the environment and getting the initial state\n","    state = env.reset()\n","    \n","    # Looping until the episode ends\n","    while True:\n","        # Choosing an action based on the current state using the policy network\n","        action, log_prob = policy_net.act(state)\n","        \n","        # Taking the action in the environment and getting the next state, reward and done flag\n","        next_state, reward, done, _ = env.step(action)\n","        \n","        # Estimating the expected return for the current state using the value network\n","        value = value_net(torch.from_numpy(state).float().unsqueeze(0)).item()\n","        \n","        # Storing the state, action, reward, log probability and value in the trajectories\n","        states.append(state)\n","        actions.append(action)\n","        rewards.append(reward)\n","        log_probs.append(log_prob)\n","        values.append(value)\n","        \n","        # Updating the episode reward\n","        episode_reward += reward\n","        \n","        # Updating the state\n","        state = next_state\n","        \n","        # Rendering the environment if it is a multiple of 100 episodes\n","        if (i + 1) % 5000 == 0:\n","            env.render()\n","        \n","        # Checking if the episode has ended\n","        if done:\n","            # Calculating the discounted returns and generalized advantages from the trajectories\n","            returns = calculate_returns(rewards)\n","            advantages = calculate_advantages(returns, values)\n","            \n","            # Updating the policy and value networks using the trajectories\n","            update_networks(states, actions, log_probs, returns, advantages)\n","            \n","            # Clearing the trajectories and statistics\n","            states.clear()\n","            actions.clear()\n","            rewards.clear()\n","            log_probs.clear()\n","            values.clear()\n","            returns.clear()\n","            advantages.clear()\n","            \n","            # Storing the episode reward in the list\n","            episode_rewards.append(episode_reward)\n","            \n","            # Printing the episode reward\n","            print(f'Episode {i + 1}: Reward = {episode_reward}')\n","            \n","            # Resetting the episode reward\n","            episode_reward = 0\n","            \n","            # Breaking out of the loop\n","            break\n","\n","# Plotting the episode rewards over time\n","import matplotlib.pyplot as plt\n","\n","plt.plot(episode_rewards)\n","plt.xlabel('Episode')\n","plt.ylabel('Reward')\n","plt.title('Episode Rewards over Time')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
